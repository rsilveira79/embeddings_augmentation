{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch, Sklearn imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0.dev20190206\n",
      "AllenNLP: 0.8.1\n"
     ]
    }
   ],
   "source": [
    "## AllenNLP\n",
    "import allennlp\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from allennlp.modules.token_embedders import BertEmbedder\n",
    "\n",
    "print(\"PyTorch: {}\".format(torch.__version__))\n",
    "print(\"AllenNLP: {}\".format(allennlp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/07/2019 07:49:05 - INFO - gensim.summarization.textcleaner -   'pattern' package not found; tag filters are not available for English\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/roberto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## NLP libs\n",
    "from nltk import download\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "download('stopwords')\n",
    "\n",
    "## Sklearn imports\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "## General libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import os, re, sys, json, requests, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text, do_stop=False, do_stem=False, do_lema = False):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing E-mails  \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', r' ', text)\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Removing Newline\n",
    "    text = text.rstrip()\n",
    "    # Removing all the stopwords\n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Stemming\n",
    "    if (do_stem==True):\n",
    "        text = gensim.parsing.preprocessing.stem_text(text)\n",
    "    \n",
    "    # Lemmatization\n",
    "    if (do_lema==True):\n",
    "        text = do_lemmatization(text)   \n",
    "        \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization function based on Spacy Library\n",
    "def lemmatizer_spacy(text):        \n",
    "    sent = []\n",
    "    doc = spacy_en(text)\n",
    "    for word in doc:\n",
    "        if word.lemma_ == \"-PRON-\":\n",
    "            sent.append(word.text)\n",
    "        else:\n",
    "            sent.append(word.lemma_)\n",
    "    return \" \".join(sent)\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset/intents_train_roberto.csv', sep=';')\n",
    "dataset.rename(index=str, columns={\"Intent\": \"intent\", \"result\": \"phrases\"},inplace=True)\n",
    "dataset['clean_text']=dataset['phrases'].apply(lambda x: transformText(x))## Suffle the dataset\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-custom-intent-engines  snips_sliced_test.pkl   SST2_capado.pkl\r\n",
      "intents_test_roberto.pkl       snips_sliced_train.pkl  SST2.pkl\r\n",
      "intents_train_roberto.csv      SST1_capado.pkl\r\n",
      "snips_dataset.csv\t       SST1.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test current model accuracy\n",
    "with open('dataset/intents_test_roberto.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dicionario: 764\n",
      "Tamanho dos labels: 169\n"
     ]
    }
   ],
   "source": [
    "## Build word vocabulary\n",
    "word_to_ix = {}\n",
    "for sent in dataset.clean_text:\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(\"Tamanho do dicionario: {}\".format(len(word_to_ix)))\n",
    "## Build label vocabulary\n",
    "label_to_ix = {}\n",
    "for label in dataset.intent:\n",
    "    for word in label.split():\n",
    "        if word not in label_to_ix:\n",
    "            label_to_ix[word]=len(label_to_ix)\n",
    "print(\"Tamanho dos labels: {}\".format(len(label_to_ix)))\n",
    "\n",
    "data_split = int(0.8*len(dataset))\n",
    "train_dataset = dataset[:data_split]\n",
    "valid_dataset = dataset[data_split:-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1981, 495)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL personBirthInformation.update - PHRASES 27\n",
      "LABEL position.delete - PHRASES 7\n",
      "LABEL personGender.update - PHRASES 12\n",
      "LABEL workerIndicativeData.verify - PHRASES 10\n",
      "LABEL compensationPlanStep.update - PHRASES 9\n",
      "LABEL jobRequisition.cancel - PHRASES 14\n",
      "LABEL payGrade.activate - PHRASES 9\n",
      "LABEL vendor.activate - PHRASES 12\n",
      "LABEL evaluation.review - PHRASES 9\n",
      "LABEL workerGender.update - PHRASES 10\n",
      "LABEL workerDisability.update - PHRASES 11\n",
      "LABEL jobOffer.evaluate - PHRASES 13\n",
      "LABEL workerHeight.update - PHRASES 13\n",
      "LABEL jobApplication.withdraw - PHRASES 10\n",
      "LABEL workerEthnicity.update - PHRASES 19\n",
      "LABEL jobApplication.update - PHRASES 11\n",
      "LABEL worker.usI9Screening.section2.complete - PHRASES 8\n",
      "LABEL jobRequisition.close - PHRASES 13\n",
      "LABEL jobFamily.deactivate - PHRASES 10\n",
      "LABEL compensationPlanStep.delete - PHRASES 11\n",
      "LABEL payGrade.delete - PHRASES 9\n",
      "LABEL jobApplicationInterest.confirm - PHRASES 10\n",
      "LABEL campus.update - PHRASES 6\n",
      "LABEL worker.usI9Screening.status.update - PHRASES 9\n",
      "LABEL evaluation.update - PHRASES 13\n",
      "LABEL payGrade.create - PHRASES 10\n",
      "LABEL compensationPlanStep.create - PHRASES 11\n",
      "LABEL internalJobApplication.submit - PHRASES 12\n",
      "LABEL workerBirthInformation.update - PHRASES 19\n",
      "LABEL paymentDate.update - PHRASES 1\n",
      "LABEL promote - PHRASES 10\n",
      "LABEL vendor.deactivate - PHRASES 8\n",
      "LABEL jobPosting.cancel - PHRASES 11\n",
      "LABEL compensationPlan.update - PHRASES 20\n",
      "LABEL compensationPlan.delete - PHRASES 13\n",
      "LABEL payGrade.update - PHRASES 8\n",
      "LABEL worker.changeJob - PHRASES 9\n",
      "LABEL helper.intents - PHRASES 13\n",
      "LABEL jobRequisitionRecruiter.assign - PHRASES 5\n",
      "LABEL legalEntity.close - PHRASES 11\n",
      "LABEL workerCitizenship.update - PHRASES 10\n",
      "LABEL workerName.update - PHRASES 6\n",
      "LABEL legalEntity.delete - PHRASES 12\n",
      "LABEL worker.changeManager - PHRASES 6\n",
      "LABEL helper.questions - PHRASES 6\n",
      "LABEL evaluation.cancel - PHRASES 12\n",
      "LABEL personVeteranStatus.update - PHRASES 12\n",
      "LABEL worker.usI9Screening.section2.generate - PHRASES 11\n",
      "LABEL personSocialNetwork.update - PHRASES 11\n",
      "LABEL workerPersonalPhoneNumber.update - PHRASES 19\n",
      "LABEL personGovernmentRegistration.update - PHRASES 21\n",
      "LABEL worker.changePosition - PHRASES 11\n",
      "LABEL workerBusinessContactInformation.update - PHRASES 9\n",
      "LABEL question.detect - PHRASES 31\n",
      "LABEL worker.usI9Screening.section1.generate - PHRASES 9\n",
      "LABEL jobReferral.hold - PHRASES 6\n",
      "LABEL jobRequisition.hold - PHRASES 16\n",
      "LABEL worker.resign - PHRASES 12\n",
      "LABEL personDisability.update - PHRASES 10\n",
      "LABEL campus.delete - PHRASES 7\n",
      "LABEL evaluation.schedule - PHRASES 12\n",
      "LABEL level.delete - PHRASES 10\n",
      "LABEL personHeight.update - PHRASES 10\n",
      "LABEL worker.terminate - PHRASES 10\n",
      "LABEL jobOffer.create - PHRASES 9\n",
      "LABEL positionRelationships.update - PHRASES 35\n",
      "LABEL workerStudentStatus.update - PHRASES 15\n",
      "LABEL workerPersonalContacts.update - PHRASES 14\n",
      "LABEL workerMaritalStatus.update - PHRASES 30\n",
      "LABEL job.delete - PHRASES 10\n",
      "LABEL assessment.take - PHRASES 9\n",
      "LABEL workerPersonalEmail.update - PHRASES 22\n",
      "LABEL jobReferral.payment - PHRASES 9\n",
      "LABEL workerDeathDate.inform - PHRASES 12\n",
      "LABEL personPersonalAddress.update - PHRASES 11\n",
      "LABEL personName.update - PHRASES 10\n",
      "LABEL level.activate - PHRASES 7\n",
      "LABEL worker.changeLevel - PHRASES 5\n",
      "LABEL band.update - PHRASES 14\n",
      "LABEL jobRequisition.activate - PHRASES 15\n",
      "LABEL demote - PHRASES 7\n",
      "LABEL jobPosting.create - PHRASES 17\n",
      "LABEL vendor.update - PHRASES 11\n",
      "LABEL personEthnicity.update - PHRASES 19\n",
      "LABEL payrollSchedulePractitioner.assign - PHRASES 1\n",
      "LABEL termination.revoke - PHRASES 7\n",
      "LABEL view.job - PHRASES 6\n",
      "LABEL personDeathDate.inform - PHRASES 13\n",
      "LABEL band.delete - PHRASES 11\n",
      "LABEL jobReferral.activate - PHRASES 10\n",
      "LABEL assessment.add - PHRASES 9\n",
      "LABEL jobPosting.update - PHRASES 8\n",
      "LABEL person.create - PHRASES 8\n",
      "LABEL personBloodGroup.update - PHRASES 19\n",
      "LABEL legalEntity.create - PHRASES 11\n",
      "LABEL jobBoard.update - PHRASES 11\n",
      "LABEL payGrade.deactivate - PHRASES 8\n",
      "LABEL jobReferral.evaluate - PHRASES 11\n",
      "LABEL job.create - PHRASES 11\n",
      "LABEL jobBoard.create - PHRASES 6\n",
      "LABEL personStudentStatus.update - PHRASES 8\n",
      "LABEL jobSearch.update - PHRASES 5\n",
      "LABEL workerPersonalAddress.update - PHRASES 21\n",
      "LABEL band.deactivate - PHRASES 8\n",
      "LABEL worker.changeLocation - PHRASES 9\n",
      "LABEL personTobaccoUsageStatus.update - PHRASES 17\n",
      "LABEL transfer - PHRASES 8\n",
      "LABEL workerBloodGroup.update - PHRASES 20\n",
      "LABEL jobBoard.delete - PHRASES 9\n",
      "LABEL worker.usI9Screening.section1.complete - PHRASES 15\n",
      "LABEL jobRequisition.update - PHRASES 10\n",
      "LABEL associateGovernmentRegistration.update - PHRASES 25\n",
      "LABEL workAssignment.create - PHRASES 6\n",
      "LABEL jobSearch.delete - PHRASES 4\n",
      "LABEL level.create - PHRASES 8\n",
      "LABEL band.create - PHRASES 11\n",
      "LABEL legalEntity.activate - PHRASES 11\n",
      "LABEL job.deactivate - PHRASES 11\n",
      "LABEL jobRequisition.create - PHRASES 18\n",
      "LABEL jobFamily.activate - PHRASES 8\n",
      "LABEL vendor.create - PHRASES 14\n",
      "LABEL compensationPlan.create - PHRASES 17\n",
      "LABEL assessment.submit - PHRASES 11\n",
      "LABEL associateGovernmentRegistration.create - PHRASES 22\n",
      "LABEL position.deactivate - PHRASES 8\n",
      "LABEL personCitizenship.update - PHRASES 10\n",
      "LABEL externalJobApplication.submit - PHRASES 6\n",
      "LABEL campus.create - PHRASES 10\n",
      "LABEL personGovernmentRegistration.create - PHRASES 18\n",
      "LABEL workerStartDate.update - PHRASES 10\n",
      "LABEL jobApplication.reject - PHRASES 9\n",
      "LABEL personLGBT.update - PHRASES 24\n",
      "LABEL jobFamily.delete - PHRASES 12\n",
      "LABEL helper.commands - PHRASES 11\n",
      "LABEL resign.revoke - PHRASES 9\n",
      "LABEL candidate.refer - PHRASES 12\n",
      "LABEL personMaritalStatus.update - PHRASES 30\n",
      "LABEL position.create - PHRASES 8\n",
      "LABEL jobOfferNegotiation.evaluate - PHRASES 8\n",
      "LABEL workerVeteranStatus.update - PHRASES 9\n",
      "LABEL position.update - PHRASES 9\n",
      "LABEL jobApplication.evaluate - PHRASES 6\n",
      "LABEL legalEntity.update - PHRASES 12\n",
      "LABEL legalEntity.deactivate - PHRASES 13\n",
      "LABEL job.update - PHRASES 11\n",
      "LABEL personPersonalContacts.update - PHRASES 16\n",
      "LABEL vendor.delete - PHRASES 13\n",
      "LABEL jobSearch.create - PHRASES 6\n",
      "LABEL personPersonalPhoneNumber.update - PHRASES 16\n",
      "LABEL my.todos - PHRASES 10\n",
      "LABEL jobApplication.cancel - PHRASES 10\n",
      "LABEL worker.changeOrganization - PHRASES 20\n",
      "LABEL personPersonalEmail.update - PHRASES 16\n",
      "LABEL job.activate - PHRASES 9\n",
      "LABEL level.deactivate - PHRASES 9\n",
      "LABEL position.activate - PHRASES 6\n",
      "LABEL band.activate - PHRASES 9\n",
      "LABEL assessment.update - PHRASES 7\n",
      "LABEL level.update - PHRASES 9\n",
      "LABEL worker.hire - PHRASES 10\n",
      "LABEL workerTobaccoUsageStatus.update - PHRASES 19\n",
      "LABEL jobFamily.create - PHRASES 11\n",
      "LABEL workerLGBT.update - PHRASES 19\n",
      "LABEL jobOffer.revoke - PHRASES 14\n",
      "LABEL jobReferral.cancel - PHRASES 11\n",
      "LABEL jobFamily.update - PHRASES 11\n",
      "LABEL workerSocialNetwork.update - PHRASES 14\n",
      "LABEL compensationPlan.end - PHRASES 10\n",
      "LABEL worker.retire - PHRASES 9\n"
     ]
    }
   ],
   "source": [
    "for i in list(set(train_dataset.intent)):\n",
    "    print(\"LABEL {} - PHRASES {}\".format(i,len(train_dataset[train_dataset.intent==i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/07/2019 07:49:07 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "## ELMo\n",
    "elmo_weights_key_path = '../vectors/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
    "elmo_config_key_path = '../vectors/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
    "\n",
    "## S3 Configs for SageMaker\n",
    "#bucket = 'adp-e-ml-notebooks-sagemaker'             \n",
    "#prefix = 'vectors'   \n",
    "#elmo_weights_key = '{}/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'.format(prefix)\n",
    "#elmo_weights_key_path = os.path.join('s3://', bucket, elmo_weights_key)\n",
    "#elmo_config_key = '{}/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'.format(prefix)\n",
    "#elmo_config_key_path = os.path.join('s3://', bucket, elmo_config_key)\n",
    "\n",
    "### Elmo Instance\n",
    "elmo = Elmo(elmo_config_key_path, \n",
    "            elmo_weights_key_path, \n",
    "            num_output_representations = 1, \n",
    "            dropout=0.3,\n",
    "            requires_grad = False)\n",
    "if torch.cuda.is_available():\n",
    "    elmo = elmo.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo(sent):\n",
    "    elmo.eval()\n",
    "    sent = [sent.split()]\n",
    "    character_ids = batch_to_ids(sent)\n",
    "    if torch.cuda.is_available():\n",
    "        character_ids = character_ids.cuda()\n",
    "    embeddings = elmo(character_ids)\n",
    "    rep = embeddings['elmo_representations'][0]\n",
    "    rep = rep.squeeze(dim=0)\n",
    "    avg = rep.mean(dim=0)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3639,  0.1719,  0.0151,  ...,  0.5625, -0.5524, -0.0254],\n",
       "       device='cuda:0', grad_fn=<MeanBackward2>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_elmo(\"testing this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intents(Dataset):\n",
    "    def __init__(self, dataframe, embed_type):\n",
    "        self.len = len(dataframe)\n",
    "        self.label_to_ix = {}\n",
    "        self.data = dataframe\n",
    "        self.embed_type = embed_type\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        phrase = self.data.clean_text[index]\n",
    "        if self.embed_type == 'bert':\n",
    "            X = get_features(phrase, max_seq_length, out_layers, zero_pad, include_special_tokens)\n",
    "        elif self.embed_type == 'w2v':        \n",
    "            X = get_avg_sentence_vector(phrase)\n",
    "        elif self.embed_type == 'elmo':\n",
    "            X = get_elmo(phrase)\n",
    "        y = label_to_ix[self.data.intent[index]]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Intents(train_dataset,\"elmo\")\n",
    "validing_set = Intents(valid_dataset, \"elmo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2246, -0.1138,  0.2224,  ..., -0.2071,  0.1066, -0.1583],\n",
       "        device='cuda:0', grad_fn=<MeanBackward2>), 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0481,  0.1871,  0.2025,  ..., -0.2850,  0.0597,  0.1245],\n",
       "        device='cuda:0', grad_fn=<MeanBackward2>), 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validing_set.__getitem__(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, inputdim, \n",
    "                        nclasses, \n",
    "                        nhidden, \n",
    "                        dropout = 0):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "        -dropout:    dropout for MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        self.inputdim = inputdim\n",
    "        self.hidden_dim = nhidden\n",
    "        self.dropout = dropout\n",
    "        self.nclasses = nclasses\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.inputdim, nhidden),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nhidden, self.nclasses),\n",
    "            )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "    def forward(self, x):\n",
    "        log_probs = self.model(x)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIM = elmo.get_output_dim()\n",
    "NUM_LABELS = len(label_to_ix)\n",
    "NHIDDEN = 256\n",
    "DROPOUT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP(inputdim = INP_DIM ,\n",
    "              nhidden = NHIDDEN,\n",
    "              nclasses = NUM_LABELS,\n",
    "              dropout = DROPOUT)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = model.to(device)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders Parameters\n",
    "params = {'batch_size': 10,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "train_loader = DataLoader(training_set, **params)\n",
    "valid_loader = DataLoader(validing_set, **params)\n",
    "# Hyperparams\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001 \n",
    "optimizer = optim.Adam(params =  model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"EPOCH -- {}\".format(epoch))\n",
    "    for i, (sent, label) in enumerate(train_loader):\n",
    "        \n",
    "        ## Step 1 - Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            sent = sent.cuda()\n",
    "            label = label.cuda()\n",
    "        \n",
    "        ## Step 2 - Run forward pass\n",
    "        output = model.forward(sent)\n",
    "        \n",
    "        ## Step 3 - Compute loss\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        ## Step 4 = Update parameters\n",
    "        optimizer.step()\n",
    "        if i%50 == 0:\n",
    "            \n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for sent, label in valid_loader:      \n",
    "                if torch.cuda.is_available():\n",
    "                    sent = sent.cuda()\n",
    "                    label = label.cuda()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                output = model.forward(sent)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += label.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted.cpu() == label.cpu()).sum()\n",
    "            accuracy = 100.00 * correct.numpy() / total\n",
    "            # Print Loss\n",
    "            print('LOSS: {}. VALID ACCURACY: {}%'.format(loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reply(phrase, model):\n",
    "    x = get_elmo(phrase)\n",
    "    logits_out = model.forward(x)\n",
    "    softmax_out = F.softmax(logits_out, dim=0).cpu()\n",
    "    _, pred_label = torch.max(softmax_out.data, 0)\n",
    "    prediction=list(label_to_ix.keys())[pred_label]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reply(\"demote worker\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reply(\"change my email\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "errors = []\n",
    "for phrase, label in test_dataset:\n",
    "    phrase_transformed = transformText(phrase)\n",
    "    #print(phrase, \"---\", phrase_transformed)\n",
    "    prediction = get_reply(phrase_transformed, model)\n",
    "    total +=1\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "    else:\n",
    "        errors.append((sent,label))\n",
    "test_accuracy = 100.00 * correct / total\n",
    "print(\"TEST ACCURACY  -- {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Augmentation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torch.nn.functional import interpolate\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_interpolation(label, num_interp_samples):\n",
    "    sentences= list(train_dataset.clean_text[train_dataset.intent == label])\n",
    "    points = np.zeros((elmo.get_output_dim(),len(sentences)))    \n",
    "    #print(len(sentences), points.shape)\n",
    "    for i,utt in enumerate(sentences):\n",
    "        points[:,i] = get_elmo(utt).detach().cpu().clone().numpy()\n",
    "    point = torch.tensor(points)\n",
    "    x = point.unsqueeze(dim=0)\n",
    "    \n",
    "    ## Random selector for which interpolated phrase to pick\n",
    "    rand_phrase = np.random.randint(num_interp_samples, size = 1)\n",
    "    \n",
    "    ## Interpolate phrases\n",
    "    interp = interpolate(x, size=(num_interp_samples), mode='linear', align_corners=True).squeeze(0).numpy().T\n",
    "    return interp[rand_phrase].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augm = get_linear_interpolation('promote', 20)\n",
    "augm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_manifold(intents, num_interp_samples, perplexity = 3):\n",
    "    \n",
    "    list_intents = {}\n",
    "    for name in intents:\n",
    "        phrases = list(dataset.clean_text[dataset.intent == name])\n",
    "        list_intents.update({name:phrases})\n",
    "    ## List all sentences\n",
    "    sentences = [item for sublist in list(list_intents.values()) for item in sublist]\n",
    "    embeddings_np = np.zeros((len(sentences), elmo.get_output_dim()))\n",
    "    for i,sent in enumerate(sentences):\n",
    "        embeddings_np[i]= get_elmo(sent).detach().cpu().numpy()\n",
    "    initial_sent_size = len(sentences)\n",
    "    \n",
    "    ## Get syntetic phrases\n",
    "    for name in list_intents:\n",
    "        interp = get_linear_interpolation(name, num_interp_samples)\n",
    "        embeddings_np = np.vstack((embeddings_np,interp))\n",
    "        syntetic_label = \"--- interp_{} ---\".format(name)\n",
    "        ## initial sizes\n",
    "        for i in range(num_interp_samples):\n",
    "            sentences.append(syntetic_label)\n",
    "    \n",
    "    ## Do TSNE and plot\n",
    "    tsne=TSNE(n_components=2,perplexity = perplexity, method ='exact',verbose=1)\n",
    "    sentences_tsne = tsne.fit_transform(embeddings_np)\n",
    "    plt.subplots(figsize=(30, 15))\n",
    "    plt.grid()\n",
    "    ## Grouping points by indexes\n",
    "    points = (sentences_tsne[:initial_sent_size,0], sentences_tsne[:initial_sent_size,1])\n",
    "    syntetic_points = (sentences_tsne[initial_sent_size:,0], sentences_tsne[initial_sent_size:,1])\n",
    "    ## Ploting points\n",
    "    plt.scatter(points[0], points[1], c='r', marker='o')\n",
    "    plt.scatter(syntetic_points[0], syntetic_points[1], c='b', marker='x')\n",
    "\n",
    "    ## Adding labels\n",
    "    for label, x, y in zip(sentences[0:initial_sent_size], points[0], points[1]):\n",
    "        font = {'size' : 10, 'weight' : 'normal'}\n",
    "        plt.rc('font', **font)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "    for label, x, y in zip(sentences[initial_sent_size:], syntetic_points[0], syntetic_points[1]):\n",
    "        font = {'size' : 18, 'weight' : 'bold'}\n",
    "        plt.rc('font', **font)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold([\"promote\", \"demote\", \"transfer\", \"band.deactivate\"], 1, perplexity = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedIntents(Dataset):\n",
    "    def __init__(self, dataframe, alpha, num_samples = 20, augmentation = True):\n",
    "        self.len = len(dataframe)\n",
    "        self.label_to_ix = {}\n",
    "        self.data = dataframe\n",
    "        self.augmentation = augmentation\n",
    "        self.num_samples = num_samples\n",
    "        self.alpha = alpha                          ## probabilty to get a real phrase vs interpolated phrase\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        phrase = self.data.clean_text[index]\n",
    "        label = self.data.intent[index]\n",
    "        y = label_to_ix[label]\n",
    "        if self.augmentation:\n",
    "            proba = np.random.binomial(1, self.alpha )\n",
    "            if proba == 1:\n",
    "                ## Real Dataset Phrases\n",
    "                X = get_elmo(phrase)\n",
    "            elif proba == 0:\n",
    "                X = torch.tensor(get_linear_interpolation(label, num_interp_samples = self.num_samples), dtype=torch.float32)\n",
    "                if torch.cuda.is_available():\n",
    "                    X = X.cuda()\n",
    "        else:\n",
    "            X = get_elmo(phrase)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = AugmentedIntents(train_dataset, alpha = 0.7, num_samples = 20, augmentation = True)\n",
    "validing_set = AugmentedIntents(valid_dataset, alpha = 0.7, num_samples = 20, augmentation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = training_set.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIM = elmo.get_output_dim()\n",
    "NUM_LABELS = len(label_to_ix)\n",
    "NHIDDEN = 256\n",
    "DROPOUT = 0\n",
    "model_2 = SimpleMLP(inputdim = INP_DIM ,\n",
    "              nhidden = NHIDDEN,\n",
    "              nclasses = NUM_LABELS,\n",
    "              dropout = DROPOUT)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model_2 = model_2.to(device)\n",
    "    model_2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders Parameters\n",
    "params = {'batch_size': 10,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "train_loader = DataLoader(training_set, **params)\n",
    "valid_loader = DataLoader(validing_set, **params)\n",
    "# Hyperparams\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001 \n",
    "optimizer = optim.Adam(params =  model_2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"EPOCH -- {}\".format(epoch))\n",
    "    for i, (sent, label) in enumerate(train_loader):        \n",
    "        ## Step 1 - Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            sent = sent.cuda()\n",
    "            label = label.cuda()\n",
    "        \n",
    "        ## Step 2 - Run forward pass\n",
    "        output = model_2.forward(sent)\n",
    "        \n",
    "        ## Step 3 - Compute loss\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        ## Step 4 = Update parameters\n",
    "        optimizer.step()\n",
    "        if i%50 == 0:\n",
    "            \n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for sent, label in valid_loader:\n",
    "                #print(sent.shape)\n",
    "                #print(len(sent))\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    sent = sent.cuda()\n",
    "                    label = label.cuda()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                output = model_2.forward(sent)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += label.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted.cpu() == label.cpu()).sum()\n",
    "            accuracy = 100.00 * correct.numpy() / total\n",
    "            # Print Loss\n",
    "            print('LOSS: {}. VALID ACCURACY: {}%'.format(loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "errors = []\n",
    "for phrase, label in test_dataset:\n",
    "    phrase_transformed = transformText(phrase)\n",
    "    prediction = get_reply(phrase_transformed, model_2)\n",
    "    total +=1\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "    else:\n",
    "        errors.append((sent,label))\n",
    "test_accuracy = 100.00 * correct / total\n",
    "print(\"TEST ACCURACY  -- {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "**BASELINE (no augmentation):**\n",
    "- Valid Accuracy % = \n",
    "- Test Accuracy % = \n",
    "\n",
    "| Alpha (Real/Sintetic) | Num Samples Interpolated | Valid Accuracy % | Test Accuracy % |\n",
    "| --- | --- | --- | --- |\n",
    "| 0.7 | 10 | 86.86 | 78.51 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
